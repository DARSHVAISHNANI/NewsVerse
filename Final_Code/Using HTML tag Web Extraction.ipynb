{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2a0169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:41:14,334 - INFO - Scraping BBC...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping latest news from multiple sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:41:19,386 - INFO - Scraping CNN...\n",
      "2025-07-16 01:41:26,891 - INFO - Scraping Reuters...\n",
      "2025-07-16 01:41:27,016 - ERROR - Error getting links from https://www.reuters.com: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/\n",
      "2025-07-16 01:41:27,019 - INFO - Scraping AP News...\n",
      "2025-07-16 01:41:46,438 - INFO - Scraping The Guardian...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting from specific URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:41:54,242 - ERROR - Error extracting from https://www.cnn.com/2024/01/15/example-article: 404 Client Error: Not Found for url: https://edition.cnn.com/2024/01/15/example-article\n",
      "2025-07-16 01:41:54,249 - INFO - Saved 13 articles to news_articles.json\n",
      "2025-07-16 01:41:54,256 - INFO - Saved 13 articles to news_articles.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 13 articles:\n",
      "\n",
      "1. NewsNews\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 01:41:15\n",
      "   URL: https://www.bbc.com/news\n",
      "   Description: The existence of the scheme could not be reported until now because of a court injunction....\n",
      "   Full Content Length: 7130 characters\n",
      "   Full Content Preview: Thousands of Afghans were moved to UK in secret scheme after data breach\n",
      "\n",
      "The existence of the scheme could not be reported until now because of a court injunction.\n",
      "\n",
      "I'm 'disappointed but not done' wi...\n",
      "\n",
      "2. NewsNews\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 01:41:16\n",
      "   URL: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
      "   Description: Mahmoud Abdul Rahman's son, Abdullah, was among six children who died at a water distribution point ...\n",
      "   Full Content Length: 5802 characters\n",
      "   Full Content Preview: Israel-Gaza war\n",
      "\n",
      "Gaza father's outrage after Israeli strike kills son 'searching for sip' at water point\n",
      "\n",
      "Mahmoud Abdul Rahman's son, Abdullah, was among six children who died at a water distribution ...\n",
      "\n",
      "3. NewsNews\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 01:41:18\n",
      "   URL: https://www.bbc.com/news/war-in-ukraine\n",
      "   Description: Kyiv's mayor is among those asking why there has to be a delay to introducing new tariffs as the fig...\n",
      "   Full Content Length: 4928 characters\n",
      "   Full Content Preview: War in Ukraine\n",
      "\n",
      "Ukrainians unimpressed by Trump's 50-day ultimatum to Putin\n",
      "\n",
      "Kyiv's mayor is among those asking why there has to be a delay to introducing new tariffs as the fighting continues.\n",
      "\n",
      "I'm '...\n",
      "\n",
      "4. CNN Newsletters\n",
      "   Source: CNN\n",
      "   Date: 2025-07-16 | Time: 01:41:21\n",
      "   URL: https://www.cnn.com/newsletters\n",
      "   Description: CNN News, delivered. Select from our newsletters below and enter your email to sign up....\n",
      "   Full Content Length: 5715 characters\n",
      "   Full Content Preview: Breaking News\n",
      "\n",
      "Be the first to know about the biggest stories as they break. Sign up for breaking news email alerts from CNN.\n",
      "\n",
      "Five Things AM\n",
      "\n",
      "We’ll summarize five stories you need to know before star...\n",
      "\n",
      "5. Election Center 2025\n",
      "   Source: CNN\n",
      "   Date: None | Time: 01:41:23\n",
      "   URL: https://www.cnn.com/election/2025\n",
      "   Description: This November, two blue states where President Donald Trump gained ground in 2024 will elect new gov...\n",
      "   Full Content Length: 3011 characters\n",
      "   Full Content Preview: This November, two blue states where President Donald Trump gained ground in 2024 will elect new governors — offering a potential preview of next year’s critical midterms. In New York City, voters wil...\n",
      "\n",
      "Total articles extracted: 13\n",
      "Data saved to news_articles.json and news_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsExtractor:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "    def extract_from_url(self, url):\n",
    "        \"\"\"Extract news article data from a single URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract article data\n",
    "            article_data = {\n",
    "                'url': url,\n",
    "                'title': self._extract_title(soup),\n",
    "                'description': self._extract_description(soup),\n",
    "                'full_content': self._extract_full_content(soup),\n",
    "                'source': self._extract_source(soup, url),\n",
    "                'date': self._extract_date(soup),\n",
    "                'time': self._extract_time(soup),\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            return article_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from {url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_title(self, soup):\n",
    "        \"\"\"Extract article title using multiple selectors\"\"\"\n",
    "        selectors = [\n",
    "            'h1',\n",
    "            '.headline',\n",
    "            '.title',\n",
    "            '[class*=\"title\"]',\n",
    "            '[class*=\"headline\"]',\n",
    "            'meta[property=\"og:title\"]',\n",
    "            'meta[name=\"twitter:title\"]',\n",
    "            'title'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                         soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get_text().strip()\n",
    "        \n",
    "        return \"Title not found\"\n",
    "    \n",
    "    def _extract_description(self, soup):\n",
    "        \"\"\"Extract article description/summary\"\"\"\n",
    "        selectors = [\n",
    "            'meta[name=\"description\"]',\n",
    "            'meta[property=\"og:description\"]',\n",
    "            'meta[name=\"twitter:description\"]',\n",
    "            '.summary',\n",
    "            '.description',\n",
    "            '.excerpt',\n",
    "            '.lead',\n",
    "            'p'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                         soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text().strip()\n",
    "                    if len(text) > 50:  # Ensure it's substantial content\n",
    "                        return text[:500] + \"...\" if len(text) > 500 else text\n",
    "        \n",
    "        return \"Description not found\"\n",
    "    \n",
    "    def _extract_full_content(self, soup):\n",
    "        \"\"\"Extract the full article content\"\"\"\n",
    "        # Common selectors for article content\n",
    "        content_selectors = [\n",
    "            'article',\n",
    "            '.article-content',\n",
    "            '.story-content',\n",
    "            '.post-content',\n",
    "            '.entry-content',\n",
    "            '.content',\n",
    "            '.article-body',\n",
    "            '.story-body',\n",
    "            '.post-body',\n",
    "            '.main-content',\n",
    "            '[class*=\"article-content\"]',\n",
    "            '[class*=\"story-content\"]',\n",
    "            '[class*=\"post-content\"]',\n",
    "            '[id*=\"article-content\"]',\n",
    "            '[id*=\"story-content\"]'\n",
    "        ]\n",
    "        \n",
    "        # Try to find the main content container\n",
    "        content_container = None\n",
    "        for selector in content_selectors:\n",
    "            content_container = soup.select_one(selector)\n",
    "            if content_container:\n",
    "                break\n",
    "        \n",
    "        if content_container:\n",
    "            # Clean up the content\n",
    "            content_text = self._clean_article_content(content_container)\n",
    "            return content_text\n",
    "        \n",
    "        # Fallback: try to extract from multiple paragraph tags\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            # Filter out navigation, footer, and other non-content paragraphs\n",
    "            content_paragraphs = []\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text().strip()\n",
    "                parent_classes = ' '.join(p.parent.get('class', []))\n",
    "                \n",
    "                # Skip if paragraph is in navigation, footer, or sidebar\n",
    "                if any(skip_class in parent_classes.lower() for skip_class in \n",
    "                       ['nav', 'footer', 'sidebar', 'menu', 'header', 'ad', 'comment']):\n",
    "                    continue\n",
    "                \n",
    "                # Skip very short paragraphs (likely not main content)\n",
    "                if len(text) > 20:\n",
    "                    content_paragraphs.append(text)\n",
    "            \n",
    "            if content_paragraphs:\n",
    "                return '\\n\\n'.join(content_paragraphs)\n",
    "        \n",
    "        return \"Full content not found\"\n",
    "    \n",
    "    def _clean_article_content(self, content_container):\n",
    "        \"\"\"Clean and format the extracted article content\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        unwanted_tags = ['script', 'style', 'nav', 'footer', 'aside', 'header', 'form']\n",
    "        for tag in unwanted_tags:\n",
    "            for element in content_container.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Remove elements with common non-content classes\n",
    "        unwanted_classes = ['ad', 'advertisement', 'social', 'share', 'comment', 'related', 'sidebar', 'navigation']\n",
    "        for class_name in unwanted_classes:\n",
    "            for element in content_container.find_all(class_=lambda x: x and any(unwanted in ' '.join(x).lower() for unwanted in unwanted_classes)):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Extract text from paragraphs, maintaining structure\n",
    "        content_parts = []\n",
    "        for element in content_container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = element.get_text().strip()\n",
    "            if text and len(text) > 10:  # Filter out very short text\n",
    "                # Add extra spacing for headings\n",
    "                if element.name.startswith('h'):\n",
    "                    content_parts.append(f\"\\n{text}\\n\")\n",
    "                else:\n",
    "                    content_parts.append(text)\n",
    "        \n",
    "        # Join content and clean up\n",
    "        full_content = '\\n\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        full_content = re.sub(r'\\n\\s*\\n', '\\n\\n', full_content)\n",
    "        full_content = re.sub(r'[ \\t]+', ' ', full_content)\n",
    "        \n",
    "        return full_content.strip()\n",
    "    \n",
    "    def _extract_source(self, soup, url):\n",
    "        \"\"\"Extract news source/publication name\"\"\"\n",
    "        # Try to get from meta tags first\n",
    "        meta_selectors = [\n",
    "            'meta[property=\"og:site_name\"]',\n",
    "            'meta[name=\"application-name\"]',\n",
    "            'meta[name=\"author\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in meta_selectors:\n",
    "            element = soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                     soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "            if element:\n",
    "                return element.get('content', '').strip()\n",
    "        \n",
    "        # Try to get from common selectors\n",
    "        selectors = [\n",
    "            '.source',\n",
    "            '.publication',\n",
    "            '.site-name',\n",
    "            '.logo',\n",
    "            'header .brand'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                return element.get_text().strip()\n",
    "        \n",
    "        # Fallback to domain name\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain.replace('www.', '')\n",
    "    \n",
    "    def _extract_date(self, soup):\n",
    "        \"\"\"Extract publication date\"\"\"\n",
    "        # Try meta tags first\n",
    "        meta_selectors = [\n",
    "            'meta[property=\"article:published_time\"]',\n",
    "            'meta[name=\"publish_date\"]',\n",
    "            'meta[name=\"date\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in meta_selectors:\n",
    "            element = soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                     soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "            if element:\n",
    "                return self._parse_datetime(element.get('content', ''))\n",
    "        \n",
    "        # Try time elements\n",
    "        time_element = soup.find('time')\n",
    "        if time_element:\n",
    "            datetime_attr = time_element.get('datetime')\n",
    "            if datetime_attr:\n",
    "                return self._parse_datetime(datetime_attr)\n",
    "        \n",
    "        # Try common date selectors\n",
    "        selectors = [\n",
    "            '.date',\n",
    "            '.published',\n",
    "            '.timestamp',\n",
    "            '[class*=\"date\"]',\n",
    "            '[class*=\"time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                return self._parse_datetime(element.get_text())\n",
    "        \n",
    "        return datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    def _extract_time(self, soup):\n",
    "        \"\"\"Extract publication time\"\"\"\n",
    "        # Similar to date extraction but focusing on time\n",
    "        time_element = soup.find('time')\n",
    "        if time_element:\n",
    "            datetime_attr = time_element.get('datetime')\n",
    "            if datetime_attr:\n",
    "                parsed_time = self._parse_datetime(datetime_attr, return_time=True)\n",
    "                if parsed_time:\n",
    "                    return parsed_time\n",
    "        \n",
    "        # Try to extract from text content\n",
    "        selectors = [\n",
    "            '.time',\n",
    "            '.timestamp',\n",
    "            '[class*=\"time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                time_text = element.get_text()\n",
    "                parsed_time = self._parse_datetime(time_text, return_time=True)\n",
    "                if parsed_time:\n",
    "                    return parsed_time\n",
    "        \n",
    "        return datetime.now().strftime('%H:%M:%S')\n",
    "    \n",
    "    def _parse_datetime(self, datetime_str, return_time=False):\n",
    "        \"\"\"Parse datetime string to extract date or time\"\"\"\n",
    "        if not datetime_str:\n",
    "            return None\n",
    "        \n",
    "        # Common datetime formats\n",
    "        formats = [\n",
    "            '%Y-%m-%dT%H:%M:%S',\n",
    "            '%Y-%m-%dT%H:%M:%SZ',\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%d/%m/%Y',\n",
    "            '%m/%d/%Y',\n",
    "            '%B %d, %Y',\n",
    "            '%d %B %Y'\n",
    "        ]\n",
    "        \n",
    "        datetime_str = datetime_str.strip()\n",
    "        \n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                parsed_dt = datetime.strptime(datetime_str, fmt)\n",
    "                if return_time:\n",
    "                    return parsed_dt.strftime('%H:%M:%S')\n",
    "                else:\n",
    "                    return parsed_dt.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Try to extract date with regex\n",
    "        date_pattern = r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})|(\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})'\n",
    "        match = re.search(date_pattern, datetime_str)\n",
    "        if match:\n",
    "            date_str = match.group()\n",
    "            try:\n",
    "                if '/' in date_str:\n",
    "                    parsed_dt = datetime.strptime(date_str, '%m/%d/%Y' if date_str.split('/')[2] > '31' else '%d/%m/%Y')\n",
    "                else:\n",
    "                    parsed_dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                \n",
    "                if return_time:\n",
    "                    return parsed_dt.strftime('%H:%M:%S')\n",
    "                else:\n",
    "                    return parsed_dt.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "class NewsAggregator:\n",
    "    def __init__(self):\n",
    "        self.extractor = NewsExtractor()\n",
    "        self.news_sources = {\n",
    "            'BBC': 'https://www.bbc.com/news',\n",
    "            'CNN': 'https://www.cnn.com',\n",
    "            'Reuters': 'https://www.reuters.com',\n",
    "            'AP News': 'https://apnews.com',\n",
    "            'The Guardian': 'https://www.theguardian.com'\n",
    "        }\n",
    "    \n",
    "    def get_article_links(self, source_url, max_links=10):\n",
    "        \"\"\"Get article links from a news source homepage\"\"\"\n",
    "        try:\n",
    "            response = self.extractor.session.get(source_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all article links\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                \n",
    "                # Convert relative URLs to absolute\n",
    "                if href.startswith('/'):\n",
    "                    href = urljoin(source_url, href)\n",
    "                \n",
    "                # Filter for article URLs (basic heuristic)\n",
    "                if (href.startswith('http') and \n",
    "                    any(keyword in href.lower() for keyword in ['article', 'news', 'story', '202']) and\n",
    "                    href not in links):\n",
    "                    links.append(href)\n",
    "                    \n",
    "                    if len(links) >= max_links:\n",
    "                        break\n",
    "            \n",
    "            return links\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting links from {source_url}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_latest_news(self, max_articles_per_source=5):\n",
    "        \"\"\"Scrape latest news from multiple sources\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        for source_name, source_url in self.news_sources.items():\n",
    "            logger.info(f\"Scraping {source_name}...\")\n",
    "            \n",
    "            # Get article links\n",
    "            article_links = self.get_article_links(source_url, max_articles_per_source)\n",
    "            \n",
    "            # Extract data from each article\n",
    "            for link in article_links:\n",
    "                article_data = self.extractor.extract_from_url(link)\n",
    "                if article_data:\n",
    "                    article_data['source'] = source_name  # Override with known source\n",
    "                    all_articles.append(article_data)\n",
    "                \n",
    "                time.sleep(1)  # Be respectful with requests\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def save_to_json(self, articles, filename='news_articles.json'):\n",
    "        \"\"\"Save articles to JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved {len(articles)} articles to {filename}\")\n",
    "    \n",
    "    def save_to_csv(self, articles, filename='news_articles.csv'):\n",
    "        \"\"\"Save articles to CSV file\"\"\"\n",
    "        if not articles:\n",
    "            return\n",
    "        \n",
    "        fieldnames = ['title', 'description', 'full_content', 'source', 'date', 'time', 'url', 'extracted_at']\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(articles)\n",
    "        \n",
    "        logger.info(f\"Saved {len(articles)} articles to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the aggregator\n",
    "    aggregator = NewsAggregator()\n",
    "    \n",
    "    # Method 1: Scrape from multiple sources\n",
    "    print(\"Scraping latest news from multiple sources...\")\n",
    "    articles = aggregator.scrape_latest_news(max_articles_per_source=3)\n",
    "    \n",
    "    # Method 2: Extract from specific URLs\n",
    "    specific_urls = [\n",
    "        \"https://www.bbc.com/news\",\n",
    "        \"https://www.cnn.com/2024/01/15/example-article\"  # Replace with actual URLs\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nExtracting from specific URLs...\")\n",
    "    for url in specific_urls:\n",
    "        article = aggregator.extractor.extract_from_url(url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nExtracted {len(articles)} articles:\")\n",
    "    for i, article in enumerate(articles[:5], 1):  # Show first 5\n",
    "        print(f\"\\n{i}. {article['title']}\")\n",
    "        print(f\"   Source: {article['source']}\")\n",
    "        print(f\"   Date: {article['date']} | Time: {article['time']}\")\n",
    "        print(f\"   URL: {article['url']}\")\n",
    "        print(f\"   Description: {article['description'][:100]}...\")\n",
    "        print(f\"   Full Content Length: {len(article['full_content'])} characters\")\n",
    "        print(f\"   Full Content Preview: {article['full_content'][:200]}...\")\n",
    "    \n",
    "    # Save to files\n",
    "    aggregator.save_to_json(articles)\n",
    "    aggregator.save_to_csv(articles)\n",
    "    \n",
    "    print(f\"\\nTotal articles extracted: {len(articles)}\")\n",
    "    print(\"Data saved to news_articles.json and news_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671dbad",
   "metadata": {},
   "source": [
    "## Title Change code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab384da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:45:56,823 - INFO - Scraping BBC...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping latest news from multiple sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:46:03,714 - INFO - Scraping CNN...\n",
      "2025-07-16 01:46:11,141 - INFO - Scraping Reuters...\n",
      "2025-07-16 01:46:11,283 - ERROR - Error getting links from https://www.reuters.com: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/\n",
      "2025-07-16 01:46:11,284 - INFO - Scraping AP News...\n",
      "2025-07-16 01:46:25,308 - INFO - Scraping The Guardian...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting from specific URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:46:31,559 - INFO - Saved 13 articles to news_articles.json\n",
      "2025-07-16 01:46:31,567 - INFO - Saved 13 articles to news_articles.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 13 articles:\n",
      "\n",
      "1. BBC News - Breaking news, video and the latest top stories from the U.S. and around the world\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 01:45:58\n",
      "   URL: https://www.bbc.com/news\n",
      "   Description: The existence of the scheme could not be reported until now because of a court injunction....\n",
      "   Full Content Length: 7130 characters\n",
      "   Full Content Preview: Thousands of Afghans were moved to UK in secret scheme after data breach\n",
      "\n",
      "The existence of the scheme could not be reported until now because of a court injunction.\n",
      "\n",
      "I'm 'disappointed but not done' wi...\n",
      "\n",
      "2. Israel Gaza war | Latest News & Updates | BBC News\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 01:46:01\n",
      "   URL: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
      "   Description: Mahmoud Abdul Rahman's son, Abdullah, was among six children who died at a water distribution point ...\n",
      "   Full Content Length: 5802 characters\n",
      "   Full Content Preview: Israel-Gaza war\n",
      "\n",
      "Gaza father's outrage after Israeli strike kills son 'searching for sip' at water point\n",
      "\n",
      "Mahmoud Abdul Rahman's son, Abdullah, was among six children who died at a water distribution ...\n",
      "\n",
      "3. Ukraine War | Latest News & Updates| BBC News\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 01:46:02\n",
      "   URL: https://www.bbc.com/news/war-in-ukraine\n",
      "   Description: Kyiv's mayor is among those asking why there has to be a delay to introducing new tariffs as the fig...\n",
      "   Full Content Length: 4928 characters\n",
      "   Full Content Preview: War in Ukraine\n",
      "\n",
      "Ukrainians unimpressed by Trump's 50-day ultimatum to Putin\n",
      "\n",
      "Kyiv's mayor is among those asking why there has to be a delay to introducing new tariffs as the fighting continues.\n",
      "\n",
      "I'm '...\n",
      "\n",
      "4. CNN newsletters: Subscribe for news, lifestyle, markets info and more | CNN\n",
      "   Source: CNN\n",
      "   Date: 2025-07-16 | Time: 01:46:05\n",
      "   URL: https://www.cnn.com/newsletters\n",
      "   Description: CNN News, delivered. Select from our newsletters below and enter your email to sign up....\n",
      "   Full Content Length: 5715 characters\n",
      "   Full Content Preview: Breaking News\n",
      "\n",
      "Be the first to know about the biggest stories as they break. Sign up for breaking news email alerts from CNN.\n",
      "\n",
      "Five Things AM\n",
      "\n",
      "We’ll summarize five stories you need to know before star...\n",
      "\n",
      "5. Arizona special primary election results live | CNN Politics\n",
      "   Source: CNN\n",
      "   Date: None | Time: 01:46:07\n",
      "   URL: https://www.cnn.com/election/2025\n",
      "   Description: This November, two blue states where President Donald Trump gained ground in 2024 will elect new gov...\n",
      "   Full Content Length: 3011 characters\n",
      "   Full Content Preview: This November, two blue states where President Donald Trump gained ground in 2024 will elect new governors — offering a potential preview of next year’s critical midterms. In New York City, voters wil...\n",
      "\n",
      "Total articles extracted: 13\n",
      "Data saved to news_articles.json and news_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsExtractor:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "    def extract_from_url(self, url):\n",
    "        \"\"\"Extract news article data from a single URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract article data\n",
    "            article_data = {\n",
    "                'url': url,\n",
    "                'title': self._extract_title(soup),\n",
    "                'description': self._extract_description(soup),\n",
    "                'full_content': self._extract_full_content(soup),\n",
    "                'source': self._extract_source(soup, url),\n",
    "                'date': self._extract_date(soup),\n",
    "                'time': self._extract_time(soup),\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            return article_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from {url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_title(self, soup):\n",
    "        \"\"\"Extract article title using multiple selectors with better filtering\"\"\"\n",
    "        \n",
    "        # Priority 1: Meta tags (usually most reliable for article titles)\n",
    "        meta_selectors = [\n",
    "            ('meta[property=\"og:title\"]', 'property', 'og:title'),\n",
    "            ('meta[name=\"twitter:title\"]', 'name', 'twitter:title'),\n",
    "            ('meta[property=\"article:title\"]', 'property', 'article:title'),\n",
    "            ('meta[name=\"title\"]', 'name', 'title')\n",
    "        ]\n",
    "        \n",
    "        for selector, attr_type, attr_value in meta_selectors:\n",
    "            element = soup.find('meta', attrs={attr_type: attr_value})\n",
    "            if element:\n",
    "                title = element.get('content', '').strip()\n",
    "                if title and self._is_valid_title(title):\n",
    "                    return title\n",
    "        \n",
    "        # Priority 2: Article-specific title selectors\n",
    "        article_title_selectors = [\n",
    "            'article h1',\n",
    "            '.article-title',\n",
    "            '.entry-title',\n",
    "            '.post-title',\n",
    "            '.story-title',\n",
    "            '.headline',\n",
    "            '.article-headline',\n",
    "            '.story-headline',\n",
    "            '[class*=\"article-title\"]',\n",
    "            '[class*=\"story-title\"]',\n",
    "            '[class*=\"post-title\"]',\n",
    "            '[class*=\"headline\"]',\n",
    "            '[id*=\"title\"]',\n",
    "            '[id*=\"headline\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in article_title_selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                title = element.get_text().strip()\n",
    "                if title and self._is_valid_title(title):\n",
    "                    return title\n",
    "        \n",
    "        # Priority 3: Main H1 tag (but filter out site titles)\n",
    "        h1_elements = soup.find_all('h1')\n",
    "        for h1 in h1_elements:\n",
    "            title = h1.get_text().strip()\n",
    "            if title and self._is_valid_title(title):\n",
    "                # Additional check: ensure it's not in header/nav/footer\n",
    "                parent_classes = ' '.join(h1.parent.get('class', []))\n",
    "                if not any(skip_class in parent_classes.lower() for skip_class in \n",
    "                          ['header', 'nav', 'footer', 'menu', 'sidebar']):\n",
    "                    return title\n",
    "        \n",
    "        # Priority 4: Any element with title-like classes\n",
    "        general_title_selectors = [\n",
    "            '.title',\n",
    "            '[class*=\"title\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in general_title_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                title = element.get_text().strip()\n",
    "                if title and self._is_valid_title(title):\n",
    "                    # Check if it's likely an article title (not navigation, etc.)\n",
    "                    parent_classes = ' '.join(element.parent.get('class', []))\n",
    "                    if not any(skip_class in parent_classes.lower() for skip_class in \n",
    "                              ['nav', 'footer', 'sidebar', 'menu', 'header']):\n",
    "                        return title\n",
    "        \n",
    "        # Last resort: HTML title tag (but clean it up)\n",
    "        title_element = soup.find('title')\n",
    "        if title_element:\n",
    "            title = title_element.get_text().strip()\n",
    "            # Clean up site name from title\n",
    "            title = self._clean_title(title)\n",
    "            if title and self._is_valid_title(title):\n",
    "                return title\n",
    "        \n",
    "        return \"Title not found\"\n",
    "    \n",
    "    def _is_valid_title(self, title):\n",
    "        \"\"\"Check if extracted title is valid (not generic site content)\"\"\"\n",
    "        if not title or len(title) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Filter out common non-title content\n",
    "        invalid_patterns = [\n",
    "            'home',\n",
    "            'news',\n",
    "            'breaking news',\n",
    "            'latest news',\n",
    "            'top stories',\n",
    "            'headlines',\n",
    "            'menu',\n",
    "            'navigation',\n",
    "            'search',\n",
    "            'subscribe',\n",
    "            'login',\n",
    "            'sign in',\n",
    "            'contact',\n",
    "            'about',\n",
    "            'privacy',\n",
    "            'terms',\n",
    "            'cookie'\n",
    "        ]\n",
    "        \n",
    "        title_lower = title.lower()\n",
    "        \n",
    "        # Check if title is just a generic term\n",
    "        if title_lower in invalid_patterns:\n",
    "            return False\n",
    "        \n",
    "        # Check for repetitive patterns like \"newsnews\" or \"bbcbbc\"\n",
    "        if len(set(title_lower.split())) < len(title_lower.split()) * 0.7:\n",
    "            return False\n",
    "        \n",
    "        # Check for overly repetitive characters\n",
    "        if any(char * 3 in title_lower for char in 'abcdefghijklmnopqrstuvwxyz'):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _clean_title(self, title):\n",
    "        \"\"\"Clean title by removing site name and other clutter\"\"\"\n",
    "        if not title:\n",
    "            return title\n",
    "        \n",
    "        # Common separators used in HTML titles\n",
    "        separators = [' - ', ' | ', ' :: ', ' › ', ' > ', ' • ', ' · ']\n",
    "        \n",
    "        # Try to split by separators and take the first (usually article title)\n",
    "        for separator in separators:\n",
    "            if separator in title:\n",
    "                parts = title.split(separator)\n",
    "                # Take the longest part (likely the article title)\n",
    "                longest_part = max(parts, key=len).strip()\n",
    "                if len(longest_part) > 10:\n",
    "                    return longest_part\n",
    "        \n",
    "        # Remove common site suffixes\n",
    "        site_suffixes = [\n",
    "            'CNN',\n",
    "            'BBC News',\n",
    "            'Reuters',\n",
    "            'AP News',\n",
    "            'The Guardian',\n",
    "            'News',\n",
    "            'Breaking News',\n",
    "            'Latest News'\n",
    "        ]\n",
    "        \n",
    "        cleaned_title = title\n",
    "        for suffix in site_suffixes:\n",
    "            if cleaned_title.endswith(suffix):\n",
    "                cleaned_title = cleaned_title[:-len(suffix)].strip()\n",
    "                break\n",
    "        \n",
    "        return cleaned_title\n",
    "    \n",
    "    def _extract_description(self, soup):\n",
    "        \"\"\"Extract article description/summary\"\"\"\n",
    "        selectors = [\n",
    "            'meta[name=\"description\"]',\n",
    "            'meta[property=\"og:description\"]',\n",
    "            'meta[name=\"twitter:description\"]',\n",
    "            '.summary',\n",
    "            '.description',\n",
    "            '.excerpt',\n",
    "            '.lead',\n",
    "            'p'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                         soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text().strip()\n",
    "                    if len(text) > 50:  # Ensure it's substantial content\n",
    "                        return text[:500] + \"...\" if len(text) > 500 else text\n",
    "        \n",
    "        return \"Description not found\"\n",
    "    \n",
    "    def _extract_full_content(self, soup):\n",
    "        \"\"\"Extract the full article content\"\"\"\n",
    "        # Common selectors for article content\n",
    "        content_selectors = [\n",
    "            'article',\n",
    "            '.article-content',\n",
    "            '.story-content',\n",
    "            '.post-content',\n",
    "            '.entry-content',\n",
    "            '.content',\n",
    "            '.article-body',\n",
    "            '.story-body',\n",
    "            '.post-body',\n",
    "            '.main-content',\n",
    "            '[class*=\"article-content\"]',\n",
    "            '[class*=\"story-content\"]',\n",
    "            '[class*=\"post-content\"]',\n",
    "            '[id*=\"article-content\"]',\n",
    "            '[id*=\"story-content\"]'\n",
    "        ]\n",
    "        \n",
    "        # Try to find the main content container\n",
    "        content_container = None\n",
    "        for selector in content_selectors:\n",
    "            content_container = soup.select_one(selector)\n",
    "            if content_container:\n",
    "                break\n",
    "        \n",
    "        if content_container:\n",
    "            # Clean up the content\n",
    "            content_text = self._clean_article_content(content_container)\n",
    "            return content_text\n",
    "        \n",
    "        # Fallback: try to extract from multiple paragraph tags\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            # Filter out navigation, footer, and other non-content paragraphs\n",
    "            content_paragraphs = []\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text().strip()\n",
    "                parent_classes = ' '.join(p.parent.get('class', []))\n",
    "                \n",
    "                # Skip if paragraph is in navigation, footer, or sidebar\n",
    "                if any(skip_class in parent_classes.lower() for skip_class in \n",
    "                       ['nav', 'footer', 'sidebar', 'menu', 'header', 'ad', 'comment']):\n",
    "                    continue\n",
    "                \n",
    "                # Skip very short paragraphs (likely not main content)\n",
    "                if len(text) > 20:\n",
    "                    content_paragraphs.append(text)\n",
    "            \n",
    "            if content_paragraphs:\n",
    "                return '\\n\\n'.join(content_paragraphs)\n",
    "        \n",
    "        return \"Full content not found\"\n",
    "    \n",
    "    def _clean_article_content(self, content_container):\n",
    "        \"\"\"Clean and format the extracted article content\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        unwanted_tags = ['script', 'style', 'nav', 'footer', 'aside', 'header', 'form']\n",
    "        for tag in unwanted_tags:\n",
    "            for element in content_container.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Remove elements with common non-content classes\n",
    "        unwanted_classes = ['ad', 'advertisement', 'social', 'share', 'comment', 'related', 'sidebar', 'navigation']\n",
    "        for class_name in unwanted_classes:\n",
    "            for element in content_container.find_all(class_=lambda x: x and any(unwanted in ' '.join(x).lower() for unwanted in unwanted_classes)):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Extract text from paragraphs, maintaining structure\n",
    "        content_parts = []\n",
    "        for element in content_container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = element.get_text().strip()\n",
    "            if text and len(text) > 10:  # Filter out very short text\n",
    "                # Add extra spacing for headings\n",
    "                if element.name.startswith('h'):\n",
    "                    content_parts.append(f\"\\n{text}\\n\")\n",
    "                else:\n",
    "                    content_parts.append(text)\n",
    "        \n",
    "        # Join content and clean up\n",
    "        full_content = '\\n\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        full_content = re.sub(r'\\n\\s*\\n', '\\n\\n', full_content)\n",
    "        full_content = re.sub(r'[ \\t]+', ' ', full_content)\n",
    "        \n",
    "        return full_content.strip()\n",
    "    \n",
    "    def _extract_source(self, soup, url):\n",
    "        \"\"\"Extract news source/publication name\"\"\"\n",
    "        # Try to get from meta tags first\n",
    "        meta_selectors = [\n",
    "            'meta[property=\"og:site_name\"]',\n",
    "            'meta[name=\"application-name\"]',\n",
    "            'meta[name=\"author\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in meta_selectors:\n",
    "            element = soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                     soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "            if element:\n",
    "                return element.get('content', '').strip()\n",
    "        \n",
    "        # Try to get from common selectors\n",
    "        selectors = [\n",
    "            '.source',\n",
    "            '.publication',\n",
    "            '.site-name',\n",
    "            '.logo',\n",
    "            'header .brand'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                return element.get_text().strip()\n",
    "        \n",
    "        # Fallback to domain name\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain.replace('www.', '')\n",
    "    \n",
    "    def _extract_date(self, soup):\n",
    "        \"\"\"Extract publication date\"\"\"\n",
    "        # Try meta tags first\n",
    "        meta_selectors = [\n",
    "            'meta[property=\"article:published_time\"]',\n",
    "            'meta[name=\"publish_date\"]',\n",
    "            'meta[name=\"date\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in meta_selectors:\n",
    "            element = soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                     soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "            if element:\n",
    "                return self._parse_datetime(element.get('content', ''))\n",
    "        \n",
    "        # Try time elements\n",
    "        time_element = soup.find('time')\n",
    "        if time_element:\n",
    "            datetime_attr = time_element.get('datetime')\n",
    "            if datetime_attr:\n",
    "                return self._parse_datetime(datetime_attr)\n",
    "        \n",
    "        # Try common date selectors\n",
    "        selectors = [\n",
    "            '.date',\n",
    "            '.published',\n",
    "            '.timestamp',\n",
    "            '[class*=\"date\"]',\n",
    "            '[class*=\"time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                return self._parse_datetime(element.get_text())\n",
    "        \n",
    "        return datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    def _extract_time(self, soup):\n",
    "        \"\"\"Extract publication time\"\"\"\n",
    "        # Similar to date extraction but focusing on time\n",
    "        time_element = soup.find('time')\n",
    "        if time_element:\n",
    "            datetime_attr = time_element.get('datetime')\n",
    "            if datetime_attr:\n",
    "                parsed_time = self._parse_datetime(datetime_attr, return_time=True)\n",
    "                if parsed_time:\n",
    "                    return parsed_time\n",
    "        \n",
    "        # Try to extract from text content\n",
    "        selectors = [\n",
    "            '.time',\n",
    "            '.timestamp',\n",
    "            '[class*=\"time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                time_text = element.get_text()\n",
    "                parsed_time = self._parse_datetime(time_text, return_time=True)\n",
    "                if parsed_time:\n",
    "                    return parsed_time\n",
    "        \n",
    "        return datetime.now().strftime('%H:%M:%S')\n",
    "    \n",
    "    def _parse_datetime(self, datetime_str, return_time=False):\n",
    "        \"\"\"Parse datetime string to extract date or time\"\"\"\n",
    "        if not datetime_str:\n",
    "            return None\n",
    "        \n",
    "        # Common datetime formats\n",
    "        formats = [\n",
    "            '%Y-%m-%dT%H:%M:%S',\n",
    "            '%Y-%m-%dT%H:%M:%SZ',\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%d/%m/%Y',\n",
    "            '%m/%d/%Y',\n",
    "            '%B %d, %Y',\n",
    "            '%d %B %Y'\n",
    "        ]\n",
    "        \n",
    "        datetime_str = datetime_str.strip()\n",
    "        \n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                parsed_dt = datetime.strptime(datetime_str, fmt)\n",
    "                if return_time:\n",
    "                    return parsed_dt.strftime('%H:%M:%S')\n",
    "                else:\n",
    "                    return parsed_dt.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Try to extract date with regex\n",
    "        date_pattern = r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})|(\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})'\n",
    "        match = re.search(date_pattern, datetime_str)\n",
    "        if match:\n",
    "            date_str = match.group()\n",
    "            try:\n",
    "                if '/' in date_str:\n",
    "                    parsed_dt = datetime.strptime(date_str, '%m/%d/%Y' if date_str.split('/')[2] > '31' else '%d/%m/%Y')\n",
    "                else:\n",
    "                    parsed_dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                \n",
    "                if return_time:\n",
    "                    return parsed_dt.strftime('%H:%M:%S')\n",
    "                else:\n",
    "                    return parsed_dt.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "class NewsAggregator:\n",
    "    def __init__(self):\n",
    "        self.extractor = NewsExtractor()\n",
    "        self.news_sources = {\n",
    "            'BBC': 'https://www.bbc.com/news',\n",
    "            'CNN': 'https://www.cnn.com',\n",
    "            'Reuters': 'https://www.reuters.com',\n",
    "            'AP News': 'https://apnews.com',\n",
    "            'The Guardian': 'https://www.theguardian.com'\n",
    "        }\n",
    "    \n",
    "    def get_article_links(self, source_url, max_links=10):\n",
    "        \"\"\"Get article links from a news source homepage\"\"\"\n",
    "        try:\n",
    "            response = self.extractor.session.get(source_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all article links\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                \n",
    "                # Convert relative URLs to absolute\n",
    "                if href.startswith('/'):\n",
    "                    href = urljoin(source_url, href)\n",
    "                \n",
    "                # Filter for article URLs (basic heuristic)\n",
    "                if (href.startswith('http') and \n",
    "                    any(keyword in href.lower() for keyword in ['article', 'news', 'story', '202']) and\n",
    "                    href not in links):\n",
    "                    links.append(href)\n",
    "                    \n",
    "                    if len(links) >= max_links:\n",
    "                        break\n",
    "            \n",
    "            return links\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting links from {source_url}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_latest_news(self, max_articles_per_source=5):\n",
    "        \"\"\"Scrape latest news from multiple sources\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        for source_name, source_url in self.news_sources.items():\n",
    "            logger.info(f\"Scraping {source_name}...\")\n",
    "            \n",
    "            # Get article links\n",
    "            article_links = self.get_article_links(source_url, max_articles_per_source)\n",
    "            \n",
    "            # Extract data from each article\n",
    "            for link in article_links:\n",
    "                article_data = self.extractor.extract_from_url(link)\n",
    "                if article_data:\n",
    "                    article_data['source'] = source_name  # Override with known source\n",
    "                    all_articles.append(article_data)\n",
    "                \n",
    "                time.sleep(1)  # Be respectful with requests\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def save_to_json(self, articles, filename='news_articles.json'):\n",
    "        \"\"\"Save articles to JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved {len(articles)} articles to {filename}\")\n",
    "    \n",
    "    def save_to_csv(self, articles, filename='news_articles.csv'):\n",
    "        \"\"\"Save articles to CSV file\"\"\"\n",
    "        if not articles:\n",
    "            return\n",
    "        \n",
    "        fieldnames = ['title', 'description', 'full_content', 'source', 'date', 'time', 'url', 'extracted_at']\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(articles)\n",
    "        \n",
    "        logger.info(f\"Saved {len(articles)} articles to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the aggregator\n",
    "    aggregator = NewsAggregator()\n",
    "    \n",
    "    # Method 1: Scrape from multiple sources\n",
    "    print(\"Scraping latest news from multiple sources...\")\n",
    "    articles = aggregator.scrape_latest_news(max_articles_per_source=3)\n",
    "    \n",
    "    # Method 2: Extract from specific URLs\n",
    "    specific_urls = [\n",
    "        \"https://www.bbc.com/news\"\n",
    "        # \"https://www.cnn.com/2024/01/15/example-article\"  # Replace with actual URLs\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nExtracting from specific URLs...\")\n",
    "    for url in specific_urls:\n",
    "        article = aggregator.extractor.extract_from_url(url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nExtracted {len(articles)} articles:\")\n",
    "    for i, article in enumerate(articles[:5], 1):  # Show first 5\n",
    "        print(f\"\\n{i}. {article['title']}\")\n",
    "        print(f\"   Source: {article['source']}\")\n",
    "        print(f\"   Date: {article['date']} | Time: {article['time']}\")\n",
    "        print(f\"   URL: {article['url']}\")\n",
    "        print(f\"   Description: {article['description'][:100]}...\")\n",
    "        print(f\"   Full Content Length: {len(article['full_content'])} characters\")\n",
    "        print(f\"   Full Content Preview: {article['full_content'][:200]}...\")\n",
    "    \n",
    "    # Save to files\n",
    "    aggregator.save_to_json(articles)\n",
    "    aggregator.save_to_csv(articles)\n",
    "    \n",
    "    print(f\"\\nTotal articles extracted: {len(articles)}\")\n",
    "    print(\"Data saved to news_articles.json and news_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e76314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 02:03:10,165 - INFO - Scraping BBC...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping latest news from multiple sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 02:03:16,839 - INFO - Scraping CNN...\n",
      "2025-07-16 02:03:29,884 - INFO - Scraping Reuters...\n",
      "2025-07-16 02:03:30,045 - ERROR - Error getting links from https://www.reuters.com: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/\n",
      "2025-07-16 02:03:30,047 - INFO - Scraping AP News...\n",
      "2025-07-16 02:03:50,470 - INFO - Scraping The Guardian...\n",
      "2025-07-16 02:03:57,197 - INFO - Scraping Fox News...\n",
      "2025-07-16 02:04:06,711 - INFO - Scraping NBC News...\n",
      "2025-07-16 02:04:15,820 - INFO - Scraping CBS News...\n",
      "2025-07-16 02:04:23,258 - INFO - Scraping ABC News...\n",
      "2025-07-16 02:04:32,035 - INFO - Scraping USA Today...\n",
      "2025-07-16 02:04:38,075 - INFO - Scraping The Indian Express...\n",
      "2025-07-16 02:04:43,655 - INFO - Scraping Times of India...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting from specific URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 02:04:51,792 - ERROR - Error extracting from https://www.cnn.com/2024/01/15/example-article: 404 Client Error: Not Found for url: https://edition.cnn.com/2024/01/15/example-article\n",
      "2025-07-16 02:04:51,799 - INFO - Saved 34 articles to news_articles.json\n",
      "2025-07-16 02:04:51,805 - INFO - Saved 34 articles to news_articles.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 34 articles:\n",
      "\n",
      "1. BBC News - Breaking news, video and the latest top stories from the U.S. and around the world\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 02:03:11\n",
      "   URL: https://www.bbc.com/news\n",
      "   Description: The existence of the scheme could not be reported until now because of a court injunction....\n",
      "   Full Content Length: 7130 characters\n",
      "   Full Content Preview: Thousands of Afghans were moved to UK in secret scheme after data breach\n",
      "\n",
      "The existence of the scheme could not be reported until now because of a court injunction.\n",
      "\n",
      "I'm 'disappointed but not done' wi...\n",
      "\n",
      "2. Israel Gaza war | Latest News & Updates | BBC News\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 02:03:13\n",
      "   URL: https://www.bbc.com/news/topics/c2vdnvdg6xxt\n",
      "   Description: Mahmoud Abdul Rahman's son, Abdullah, was among six children who died at a water distribution point ...\n",
      "   Full Content Length: 5802 characters\n",
      "   Full Content Preview: Israel-Gaza war\n",
      "\n",
      "Gaza father's outrage after Israeli strike kills son 'searching for sip' at water point\n",
      "\n",
      "Mahmoud Abdul Rahman's son, Abdullah, was among six children who died at a water distribution ...\n",
      "\n",
      "3. Ukraine War | Latest News & Updates| BBC News\n",
      "   Source: BBC\n",
      "   Date: 2025-07-16 | Time: 02:03:15\n",
      "   URL: https://www.bbc.com/news/war-in-ukraine\n",
      "   Description: Kyiv's mayor is among those asking why there has to be a delay to introducing new tariffs as the fig...\n",
      "   Full Content Length: 4928 characters\n",
      "   Full Content Preview: War in Ukraine\n",
      "\n",
      "Ukrainians unimpressed by Trump's 50-day ultimatum to Putin\n",
      "\n",
      "Kyiv's mayor is among those asking why there has to be a delay to introducing new tariffs as the fighting continues.\n",
      "\n",
      "I'm '...\n",
      "\n",
      "4. CNN newsletters: Subscribe for news, lifestyle, markets info and more | CNN\n",
      "   Source: CNN\n",
      "   Date: 2025-07-16 | Time: 02:03:19\n",
      "   URL: https://www.cnn.com/newsletters\n",
      "   Description: CNN News, delivered. Select from our newsletters below and enter your email to sign up....\n",
      "   Full Content Length: 5715 characters\n",
      "   Full Content Preview: Breaking News\n",
      "\n",
      "Be the first to know about the biggest stories as they break. Sign up for breaking news email alerts from CNN.\n",
      "\n",
      "Five Things AM\n",
      "\n",
      "We’ll summarize five stories you need to know before star...\n",
      "\n",
      "5. Arizona special primary election results live | CNN Politics\n",
      "   Source: CNN\n",
      "   Date: None | Time: 02:03:26\n",
      "   URL: https://www.cnn.com/election/2025\n",
      "   Description: This November, two blue states where President Donald Trump gained ground in 2024 will elect new gov...\n",
      "   Full Content Length: 3011 characters\n",
      "   Full Content Preview: This November, two blue states where President Donald Trump gained ground in 2024 will elect new governors — offering a potential preview of next year’s critical midterms. In New York City, voters wil...\n",
      "\n",
      "Total articles extracted: 34\n",
      "Data saved to news_articles.json and news_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewsExtractor:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "    def extract_from_url(self, url):\n",
    "        \"\"\"Extract news article data from a single URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract article data\n",
    "            article_data = {\n",
    "                'url': url,\n",
    "                'title': self._extract_title(soup),\n",
    "                'description': self._extract_description(soup),\n",
    "                'full_content': self._extract_full_content(soup),\n",
    "                'source': self._extract_source(soup, url),\n",
    "                'date': self._extract_date(soup),\n",
    "                'time': self._extract_time(soup),\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            return article_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from {url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_title(self, soup):\n",
    "        \"\"\"Extract article title using multiple selectors with better filtering\"\"\"\n",
    "        \n",
    "        # Priority 1: Meta tags (usually most reliable for article titles)\n",
    "        meta_selectors = [\n",
    "            ('meta[property=\"og:title\"]', 'property', 'og:title'),\n",
    "            ('meta[name=\"twitter:title\"]', 'name', 'twitter:title'),\n",
    "            ('meta[property=\"article:title\"]', 'property', 'article:title'),\n",
    "            ('meta[name=\"title\"]', 'name', 'title')\n",
    "        ]\n",
    "        \n",
    "        for selector, attr_type, attr_value in meta_selectors:\n",
    "            element = soup.find('meta', attrs={attr_type: attr_value})\n",
    "            if element:\n",
    "                title = element.get('content', '').strip()\n",
    "                if title and self._is_valid_title(title):\n",
    "                    return title\n",
    "        \n",
    "        # Priority 2: Article-specific title selectors\n",
    "        article_title_selectors = [\n",
    "            'article h1',\n",
    "            '.article-title',\n",
    "            '.entry-title',\n",
    "            '.post-title',\n",
    "            '.story-title',\n",
    "            '.headline',\n",
    "            '.article-headline',\n",
    "            '.story-headline',\n",
    "            '[class*=\"article-title\"]',\n",
    "            '[class*=\"story-title\"]',\n",
    "            '[class*=\"post-title\"]',\n",
    "            '[class*=\"headline\"]',\n",
    "            '[id*=\"title\"]',\n",
    "            '[id*=\"headline\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in article_title_selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                title = element.get_text().strip()\n",
    "                if title and self._is_valid_title(title):\n",
    "                    return title\n",
    "        \n",
    "        # Priority 3: Main H1 tag (but filter out site titles)\n",
    "        h1_elements = soup.find_all('h1')\n",
    "        for h1 in h1_elements:\n",
    "            title = h1.get_text().strip()\n",
    "            if title and self._is_valid_title(title):\n",
    "                # Additional check: ensure it's not in header/nav/footer\n",
    "                parent_classes = ' '.join(h1.parent.get('class', []))\n",
    "                if not any(skip_class in parent_classes.lower() for skip_class in \n",
    "                          ['header', 'nav', 'footer', 'menu', 'sidebar']):\n",
    "                    return title\n",
    "        \n",
    "        # Priority 4: Any element with title-like classes\n",
    "        general_title_selectors = [\n",
    "            '.title',\n",
    "            '[class*=\"title\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in general_title_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                title = element.get_text().strip()\n",
    "                if title and self._is_valid_title(title):\n",
    "                    # Check if it's likely an article title (not navigation, etc.)\n",
    "                    parent_classes = ' '.join(element.parent.get('class', []))\n",
    "                    if not any(skip_class in parent_classes.lower() for skip_class in \n",
    "                              ['nav', 'footer', 'sidebar', 'menu', 'header']):\n",
    "                        return title\n",
    "        \n",
    "        # Last resort: HTML title tag (but clean it up)\n",
    "        title_element = soup.find('title')\n",
    "        if title_element:\n",
    "            title = title_element.get_text().strip()\n",
    "            # Clean up site name from title\n",
    "            title = self._clean_title(title)\n",
    "            if title and self._is_valid_title(title):\n",
    "                return title\n",
    "        \n",
    "        return \"Title not found\"\n",
    "    \n",
    "    def _is_valid_title(self, title):\n",
    "        \"\"\"Check if extracted title is valid (not generic site content)\"\"\"\n",
    "        if not title or len(title) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Filter out common non-title content\n",
    "        invalid_patterns = [\n",
    "            'home',\n",
    "            'news',\n",
    "            'breaking news',\n",
    "            'latest news',\n",
    "            'top stories',\n",
    "            'headlines',\n",
    "            'menu',\n",
    "            'navigation',\n",
    "            'search',\n",
    "            'subscribe',\n",
    "            'login',\n",
    "            'sign in',\n",
    "            'contact',\n",
    "            'about',\n",
    "            'privacy',\n",
    "            'terms',\n",
    "            'cookie'\n",
    "        ]\n",
    "        \n",
    "        title_lower = title.lower()\n",
    "        \n",
    "        # Check if title is just a generic term\n",
    "        if title_lower in invalid_patterns:\n",
    "            return False\n",
    "        \n",
    "        # Check for repetitive patterns like \"newsnews\" or \"bbcbbc\"\n",
    "        if len(set(title_lower.split())) < len(title_lower.split()) * 0.7:\n",
    "            return False\n",
    "        \n",
    "        # Check for overly repetitive characters\n",
    "        if any(char * 3 in title_lower for char in 'abcdefghijklmnopqrstuvwxyz'):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _clean_title(self, title):\n",
    "        \"\"\"Clean title by removing site name and other clutter\"\"\"\n",
    "        if not title:\n",
    "            return title\n",
    "        \n",
    "        # Common separators used in HTML titles\n",
    "        separators = [' - ', ' | ', ' :: ', ' › ', ' > ', ' • ', ' · ']\n",
    "        \n",
    "        # Try to split by separators and take the first (usually article title)\n",
    "        for separator in separators:\n",
    "            if separator in title:\n",
    "                parts = title.split(separator)\n",
    "                # Take the longest part (likely the article title)\n",
    "                longest_part = max(parts, key=len).strip()\n",
    "                if len(longest_part) > 10:\n",
    "                    return longest_part\n",
    "        \n",
    "        # Remove common site suffixes\n",
    "        site_suffixes = [\n",
    "            'CNN',\n",
    "            'BBC News',\n",
    "            'Reuters',\n",
    "            'AP News',\n",
    "            'The Guardian',\n",
    "            'News',\n",
    "            'Breaking News',\n",
    "            'Latest News'\n",
    "        ]\n",
    "        \n",
    "        cleaned_title = title\n",
    "        for suffix in site_suffixes:\n",
    "            if cleaned_title.endswith(suffix):\n",
    "                cleaned_title = cleaned_title[:-len(suffix)].strip()\n",
    "                break\n",
    "        \n",
    "        return cleaned_title\n",
    "    \n",
    "    def _extract_description(self, soup):\n",
    "        \"\"\"Extract article description/summary\"\"\"\n",
    "        selectors = [\n",
    "            'meta[name=\"description\"]',\n",
    "            'meta[property=\"og:description\"]',\n",
    "            'meta[name=\"twitter:description\"]',\n",
    "            '.summary',\n",
    "            '.description',\n",
    "            '.excerpt',\n",
    "            '.lead',\n",
    "            'p'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                         soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text().strip()\n",
    "                    if len(text) > 50:  # Ensure it's substantial content\n",
    "                        return text[:500] + \"...\" if len(text) > 500 else text\n",
    "        \n",
    "        return \"Description not found\"\n",
    "    \n",
    "    def _extract_full_content(self, soup):\n",
    "        \"\"\"Extract the full article content\"\"\"\n",
    "        # Common selectors for article content\n",
    "        content_selectors = [\n",
    "            'article',\n",
    "            '.article-content',\n",
    "            '.story-content',\n",
    "            '.post-content',\n",
    "            '.entry-content',\n",
    "            '.content',\n",
    "            '.article-body',\n",
    "            '.story-body',\n",
    "            '.post-body',\n",
    "            '.main-content',\n",
    "            '[class*=\"article-content\"]',\n",
    "            '[class*=\"story-content\"]',\n",
    "            '[class*=\"post-content\"]',\n",
    "            '[id*=\"article-content\"]',\n",
    "            '[id*=\"story-content\"]'\n",
    "        ]\n",
    "        \n",
    "        # Try to find the main content container\n",
    "        content_container = None\n",
    "        for selector in content_selectors:\n",
    "            content_container = soup.select_one(selector)\n",
    "            if content_container:\n",
    "                break\n",
    "        \n",
    "        if content_container:\n",
    "            # Clean up the content\n",
    "            content_text = self._clean_article_content(content_container)\n",
    "            return content_text\n",
    "        \n",
    "        # Fallback: try to extract from multiple paragraph tags\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            # Filter out navigation, footer, and other non-content paragraphs\n",
    "            content_paragraphs = []\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text().strip()\n",
    "                parent_classes = ' '.join(p.parent.get('class', []))\n",
    "                \n",
    "                # Skip if paragraph is in navigation, footer, or sidebar\n",
    "                if any(skip_class in parent_classes.lower() for skip_class in \n",
    "                       ['nav', 'footer', 'sidebar', 'menu', 'header', 'ad', 'comment']):\n",
    "                    continue\n",
    "                \n",
    "                # Skip very short paragraphs (likely not main content)\n",
    "                if len(text) > 20:\n",
    "                    content_paragraphs.append(text)\n",
    "            \n",
    "            if content_paragraphs:\n",
    "                return '\\n\\n'.join(content_paragraphs)\n",
    "        \n",
    "        return \"Full content not found\"\n",
    "    \n",
    "    def _clean_article_content(self, content_container):\n",
    "        \"\"\"Clean and format the extracted article content\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        unwanted_tags = ['script', 'style', 'nav', 'footer', 'aside', 'header', 'form']\n",
    "        for tag in unwanted_tags:\n",
    "            for element in content_container.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Remove elements with common non-content classes\n",
    "        unwanted_classes = ['ad', 'advertisement', 'social', 'share', 'comment', 'related', 'sidebar', 'navigation']\n",
    "        for class_name in unwanted_classes:\n",
    "            for element in content_container.find_all(class_=lambda x: x and any(unwanted in ' '.join(x).lower() for unwanted in unwanted_classes)):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Extract text from paragraphs, maintaining structure\n",
    "        content_parts = []\n",
    "        for element in content_container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = element.get_text().strip()\n",
    "            if text and len(text) > 10:  # Filter out very short text\n",
    "                # Add extra spacing for headings\n",
    "                if element.name.startswith('h'):\n",
    "                    content_parts.append(f\"\\n{text}\\n\")\n",
    "                else:\n",
    "                    content_parts.append(text)\n",
    "        \n",
    "        # Join content and clean up\n",
    "        full_content = '\\n\\n'.join(content_parts)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        full_content = re.sub(r'\\n\\s*\\n', '\\n\\n', full_content)\n",
    "        full_content = re.sub(r'[ \\t]+', ' ', full_content)\n",
    "        \n",
    "        return full_content.strip()\n",
    "    \n",
    "    def _extract_source(self, soup, url):\n",
    "        \"\"\"Extract news source/publication name\"\"\"\n",
    "        # Try to get from meta tags first\n",
    "        meta_selectors = [\n",
    "            'meta[property=\"og:site_name\"]',\n",
    "            'meta[name=\"application-name\"]',\n",
    "            'meta[name=\"author\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in meta_selectors:\n",
    "            element = soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                     soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "            if element:\n",
    "                return element.get('content', '').strip()\n",
    "        \n",
    "        # Try to get from common selectors\n",
    "        selectors = [\n",
    "            '.source',\n",
    "            '.publication',\n",
    "            '.site-name',\n",
    "            '.logo',\n",
    "            'header .brand'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                return element.get_text().strip()\n",
    "        \n",
    "        # Fallback to domain name\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain.replace('www.', '')\n",
    "    \n",
    "    def _extract_date(self, soup):\n",
    "        \"\"\"Extract publication date\"\"\"\n",
    "        # Try meta tags first\n",
    "        meta_selectors = [\n",
    "            'meta[property=\"article:published_time\"]',\n",
    "            'meta[name=\"publish_date\"]',\n",
    "            'meta[name=\"date\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in meta_selectors:\n",
    "            element = soup.find('meta', attrs={'property': selector.split('[')[1].split('=')[1].strip('\"')}) or \\\n",
    "                     soup.find('meta', attrs={'name': selector.split('[')[1].split('=')[1].strip('\"')})\n",
    "            if element:\n",
    "                return self._parse_datetime(element.get('content', ''))\n",
    "        \n",
    "        # Try time elements\n",
    "        time_element = soup.find('time')\n",
    "        if time_element:\n",
    "            datetime_attr = time_element.get('datetime')\n",
    "            if datetime_attr:\n",
    "                return self._parse_datetime(datetime_attr)\n",
    "        \n",
    "        # Try common date selectors\n",
    "        selectors = [\n",
    "            '.date',\n",
    "            '.published',\n",
    "            '.timestamp',\n",
    "            '[class*=\"date\"]',\n",
    "            '[class*=\"time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                return self._parse_datetime(element.get_text())\n",
    "        \n",
    "        return datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    def _extract_time(self, soup):\n",
    "        \"\"\"Extract publication time\"\"\"\n",
    "        # Similar to date extraction but focusing on time\n",
    "        time_element = soup.find('time')\n",
    "        if time_element:\n",
    "            datetime_attr = time_element.get('datetime')\n",
    "            if datetime_attr:\n",
    "                parsed_time = self._parse_datetime(datetime_attr, return_time=True)\n",
    "                if parsed_time:\n",
    "                    return parsed_time\n",
    "        \n",
    "        # Try to extract from text content\n",
    "        selectors = [\n",
    "            '.time',\n",
    "            '.timestamp',\n",
    "            '[class*=\"time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                time_text = element.get_text()\n",
    "                parsed_time = self._parse_datetime(time_text, return_time=True)\n",
    "                if parsed_time:\n",
    "                    return parsed_time\n",
    "        \n",
    "        return datetime.now().strftime('%H:%M:%S')\n",
    "    \n",
    "    def _parse_datetime(self, datetime_str, return_time=False):\n",
    "        \"\"\"Parse datetime string to extract date or time\"\"\"\n",
    "        if not datetime_str:\n",
    "            return None\n",
    "        \n",
    "        # Common datetime formats\n",
    "        formats = [\n",
    "            '%Y-%m-%dT%H:%M:%S',\n",
    "            '%Y-%m-%dT%H:%M:%SZ',\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%d/%m/%Y',\n",
    "            '%m/%d/%Y',\n",
    "            '%B %d, %Y',\n",
    "            '%d %B %Y'\n",
    "        ]\n",
    "        \n",
    "        datetime_str = datetime_str.strip()\n",
    "        \n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                parsed_dt = datetime.strptime(datetime_str, fmt)\n",
    "                if return_time:\n",
    "                    return parsed_dt.strftime('%H:%M:%S')\n",
    "                else:\n",
    "                    return parsed_dt.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Try to extract date with regex\n",
    "        date_pattern = r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})|(\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})'\n",
    "        match = re.search(date_pattern, datetime_str)\n",
    "        if match:\n",
    "            date_str = match.group()\n",
    "            try:\n",
    "                if '/' in date_str:\n",
    "                    parsed_dt = datetime.strptime(date_str, '%m/%d/%Y' if date_str.split('/')[2] > '31' else '%d/%m/%Y')\n",
    "                else:\n",
    "                    parsed_dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                \n",
    "                if return_time:\n",
    "                    return parsed_dt.strftime('%H:%M:%S')\n",
    "                else:\n",
    "                    return parsed_dt.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "class NewsAggregator:\n",
    "    def __init__(self):\n",
    "        self.extractor = NewsExtractor()\n",
    "        self.news_sources = {\n",
    "            'BBC': 'https://www.bbc.com/news',\n",
    "            'CNN': 'https://www.cnn.com',\n",
    "            'Reuters': 'https://www.reuters.com',\n",
    "            'AP News': 'https://apnews.com',\n",
    "            'The Guardian': 'https://www.theguardian.com',\n",
    "            'Fox News': 'https://www.foxnews.com',\n",
    "            'NBC News': 'https://www.nbcnews.com',\n",
    "            'CBS News': 'https://www.cbsnews.com',\n",
    "            'ABC News': 'https://abcnews.go.com',\n",
    "            'USA Today': 'https://www.usatoday.com',\n",
    "            'The Indian Express': 'https://indianexpress.com',\n",
    "            'Times of India': 'https://timesofindia.indiatimes.com'\n",
    "        }\n",
    "    \n",
    "    def get_article_links(self, source_url, max_links=10):\n",
    "        \"\"\"Get article links from a news source homepage\"\"\"\n",
    "        try:\n",
    "            response = self.extractor.session.get(source_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all article links\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                \n",
    "                # Convert relative URLs to absolute\n",
    "                if href.startswith('/'):\n",
    "                    href = urljoin(source_url, href)\n",
    "                \n",
    "                # Filter for article URLs (basic heuristic)\n",
    "                if (href.startswith('http') and \n",
    "                    any(keyword in href.lower() for keyword in ['article', 'news', 'story', '202']) and\n",
    "                    href not in links):\n",
    "                    links.append(href)\n",
    "                    \n",
    "                    if len(links) >= max_links:\n",
    "                        break\n",
    "            \n",
    "            return links\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting links from {source_url}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_latest_news(self, max_articles_per_source=5):\n",
    "        \"\"\"Scrape latest news from multiple sources\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        for source_name, source_url in self.news_sources.items():\n",
    "            logger.info(f\"Scraping {source_name}...\")\n",
    "            \n",
    "            # Get article links\n",
    "            article_links = self.get_article_links(source_url, max_articles_per_source)\n",
    "            \n",
    "            # Extract data from each article\n",
    "            for link in article_links:\n",
    "                article_data = self.extractor.extract_from_url(link)\n",
    "                if article_data:\n",
    "                    article_data['source'] = source_name  # Override with known source\n",
    "                    all_articles.append(article_data)\n",
    "                \n",
    "                time.sleep(1)  # Be respectful with requests\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def save_to_json(self, articles, filename='news_articles.json'):\n",
    "        \"\"\"Save articles to JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved {len(articles)} articles to {filename}\")\n",
    "    \n",
    "    def save_to_csv(self, articles, filename='news_articles.csv'):\n",
    "        \"\"\"Save articles to CSV file\"\"\"\n",
    "        if not articles:\n",
    "            return\n",
    "        \n",
    "        fieldnames = ['title', 'description', 'full_content', 'source', 'date', 'time', 'url', 'extracted_at']\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(articles)\n",
    "        \n",
    "        logger.info(f\"Saved {len(articles)} articles to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the aggregator\n",
    "    aggregator = NewsAggregator()\n",
    "    \n",
    "    # Method 1: Scrape from multiple sources\n",
    "    print(\"Scraping latest news from multiple sources...\")\n",
    "    articles = aggregator.scrape_latest_news(max_articles_per_source=3)\n",
    "    \n",
    "    # Method 2: Extract from specific URLs\n",
    "    specific_urls = [\n",
    "        \"https://www.bbc.com/news\",\n",
    "        \"https://www.cnn.com/2024/01/15/example-article\"  # Replace with actual URLs\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nExtracting from specific URLs...\")\n",
    "    for url in specific_urls:\n",
    "        article = aggregator.extractor.extract_from_url(url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nExtracted {len(articles)} articles:\")\n",
    "    for i, article in enumerate(articles[:5], 1):  # Show first 5\n",
    "        print(f\"\\n{i}. {article['title']}\")\n",
    "        print(f\"   Source: {article['source']}\")\n",
    "        print(f\"   Date: {article['date']} | Time: {article['time']}\")\n",
    "        print(f\"   URL: {article['url']}\")\n",
    "        print(f\"   Description: {article['description'][:100]}...\")\n",
    "        print(f\"   Full Content Length: {len(article['full_content'])} characters\")\n",
    "        print(f\"   Full Content Preview: {article['full_content'][:200]}...\")\n",
    "    \n",
    "    # Save to files\n",
    "    aggregator.save_to_json(articles)\n",
    "    aggregator.save_to_csv(articles)\n",
    "    \n",
    "    print(f\"\\nTotal articles extracted: {len(articles)}\")\n",
    "    print(\"Data saved to news_articles.json and news_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386a0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62eb2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_make_request', 'api_key', 'close', 'crawl', 'from_env', 'get_crawl', 'get_credits', 'get_markdownify', 'get_searchscraper', 'get_smartscraper', 'headers', 'markdownify', 'max_retries', 'retry_delay', 'searchscraper', 'session', 'smartscraper', 'submit_feedback', 'timeout']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scrapegraph_py import Client\n",
    "client = Client(api_key=\"sgai-3e5b40b7-ae60-490b-83f2-629a8c2a2891\")\n",
    "\n",
    "print(dir(client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9420cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
