{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4acae5e",
   "metadata": {},
   "source": [
    "### Title Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from newspaper import Article\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_google_news_rss(query=\"latest\", language=\"en\"):\n",
    "    query = query.replace(\" \", \"+\")\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={query}&hl={language}&gl=US&ceid=US:{language}\"\n",
    "    \n",
    "    feed = feedparser.parse(rss_url)\n",
    "    articles = []\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        try:\n",
    "            # Extract pub date\n",
    "            pub_date = datetime(*entry.published_parsed[:6]) if \"published_parsed\" in entry else datetime.now()\n",
    "            formatted_date = pub_date.strftime(\"%Y-%m-%d\")\n",
    "            formatted_time = pub_date.strftime(\"%H:%M:%S\")\n",
    "\n",
    "            # Clean source\n",
    "            source = entry.source.title if \"source\" in entry else \"Unknown\"\n",
    "\n",
    "            # Fallback values\n",
    "            title = entry.title\n",
    "            summary = entry.summary if \"summary\" in entry else \"\"\n",
    "\n",
    "            # Try to get better content from the real article\n",
    "            article = Article(entry.link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            if article.title and article.title.lower() != \"google news\":\n",
    "                title = article.title\n",
    "            if article.text:\n",
    "                summary = article.text[:300]\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"description\": summary.strip(),\n",
    "                \"source\": source,\n",
    "                \"date\": formatted_date,\n",
    "                \"time\": formatted_time,\n",
    "                \"url\": entry.link\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {entry.link} â€” {e}\")\n",
    "            continue\n",
    "\n",
    "    return articles\n",
    "\n",
    "# âœ… Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    news_data = extract_google_news_rss(\"AI news\")\n",
    "    for news in news_data[:5]:\n",
    "        print(f\"Title: {news['title']}\")\n",
    "        print(f\"Description: {news['description']}\")\n",
    "        print(f\"Source: {news['source']}\")\n",
    "        print(f\"Date: {news['date']}\")\n",
    "        print(f\"Time: {news['time']}\")\n",
    "        print(f\"URL: {news['url']}\")\n",
    "        print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999389f3",
   "metadata": {},
   "source": [
    "### Description Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "def resolve_google_redirect(google_url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "        }\n",
    "        response = requests.get(google_url, headers=headers, timeout=10, allow_redirects=True)\n",
    "        return response.url\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to resolve redirect: {google_url} â€” {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_google_news(query=\"AI\", language=\"en\"):\n",
    "    query = query.replace(\" \", \"+\")\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={query}&hl={language}&gl=US&ceid=US:{language}\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    articles = []\n",
    "\n",
    "    config = Config()\n",
    "    config.browser_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    config.request_timeout = 10\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        try:\n",
    "            title = entry.title\n",
    "            source = entry.source.title if \"source\" in entry else \"Unknown\"\n",
    "            date_published = datetime(*entry.published_parsed[:6])\n",
    "            formatted_date = date_published.strftime(\"%Y-%m-%d\")\n",
    "            formatted_time = date_published.strftime(\"%H:%M:%S\")\n",
    "\n",
    "            google_link = entry.link\n",
    "\n",
    "            # ðŸ” Resolve to real URL\n",
    "            real_url = resolve_google_redirect(google_link)\n",
    "            if not real_url or \"news.google.com\" in urlparse(real_url).netloc:\n",
    "                print(f\"âš ï¸ Skipping unresolved: {google_link}\")\n",
    "                continue\n",
    "\n",
    "            # ðŸ” Extract content using newspaper\n",
    "            article = Article(real_url, config=config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            if article.title and article.title.lower() != \"google news\":\n",
    "                title = article.title\n",
    "\n",
    "            text = article.text.strip() if article.text else \"âš ï¸ No content\"\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"description\": text,\n",
    "                \"source\": source,\n",
    "                \"date\": formatted_date,\n",
    "                \"time\": formatted_time,\n",
    "                \"url\": real_url\n",
    "            })\n",
    "\n",
    "            # optional sleep to avoid being rate-limited\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with article: {entry.link} â€” {e}\")\n",
    "            continue\n",
    "\n",
    "    return articles\n",
    "\n",
    "# âœ… Run it\n",
    "if __name__ == \"__main__\":\n",
    "    results = extract_google_news(\"AI technology\")\n",
    "    for news in results[:5]:\n",
    "        print(f\"Title: {news['title']}\")\n",
    "        print(f\"Source: {news['source']}\")\n",
    "        print(f\"Date: {news['date']} | Time: {news['time']}\")\n",
    "        print(f\"URL: {news['url']}\")\n",
    "        print(f\"\\nFull Description:\\n{news['description']}\\n\")\n",
    "        print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e10dc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import trafilatura\n",
    "\n",
    "def resolve_google_redirect(google_url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(google_url, headers=headers, timeout=10, allow_redirects=True)\n",
    "        return response.url\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Redirect fail: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_google_news_trafilatura(query=\"AI news\", language=\"en\"):\n",
    "    query = query.replace(\" \", \"+\")\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={query}&hl={language}&gl=US&ceid=US:{language}\"\n",
    "\n",
    "    feed = feedparser.parse(rss_url)\n",
    "    articles = []\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        try:\n",
    "            title = entry.title\n",
    "            source = entry.source.title if \"source\" in entry else \"Unknown\"\n",
    "            pub_date = datetime(*entry.published_parsed[:6])\n",
    "            formatted_date = pub_date.strftime(\"%Y-%m-%d\")\n",
    "            formatted_time = pub_date.strftime(\"%H:%M:%S\")\n",
    "\n",
    "            google_link = entry.link\n",
    "            real_url = resolve_google_redirect(google_link)\n",
    "            if not real_url or \"news.google.com\" in real_url:\n",
    "                continue\n",
    "\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "            resp = requests.get(real_url, headers=headers, timeout=10)\n",
    "            downloaded = trafilatura.extract(resp.text)\n",
    "\n",
    "            full_text = downloaded.strip() if downloaded else \"âš ï¸ No content extracted.\"\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"description\": full_text,\n",
    "                \"source\": source,\n",
    "                \"date\": formatted_date,\n",
    "                \"time\": formatted_time,\n",
    "                \"url\": real_url\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return articles\n",
    "\n",
    "# âœ… Example run\n",
    "if __name__ == \"__main__\":\n",
    "    results = extract_google_news_trafilatura(\"AI chip ban China\")\n",
    "    for news in results[:3]:\n",
    "        print(f\"Title: {news['title']}\")\n",
    "        print(f\"Source: {news['source']}\")\n",
    "        print(f\"Date: {news['date']} | Time: {news['time']}\")\n",
    "        print(f\"URL: {news['url']}\")\n",
    "        print(f\"\\nFull Description:\\n{news['description']}\")\n",
    "        print(\"=\" * 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "381b547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Nvidia says it will restart sales of a key AI chip to China, in a reversal of US restrictions - CNN\n",
      "Source: CNN\n",
      "Date: 2025-07-15 | Time: 17:41:00\n",
      "URL: https://news.google.com/rss/articles/CBMijwFBVV95cUxNMUJtemdGTzdFNGxfVlE1TURscVBCd3Rzb0xNSkhqNnVYc0I3N1YxOG13QVZReXlSZEZqdmJHMExMLW9ZeEpQRDR2eE9TSnp3QWItUTVHOHBwWTlOaTZ3SGxBN3BPZmJIZ2J1RzlhZWdoazk1Z21naU95MnJkOHEtWkZIMU5PZVBGaUloVGZhTdIBlAFBVV95cUxQSGRfOGNoQnVMbFRTV1c5WG1ydWQ3Tnp1Q0htQWJmbFktWUxmMnQxUFZWNS16QTFjd1Y5LUNjclNzc1lZOW5NWjl1S0c1NTl1dE5hcUdwRWU5OXF4aWFUZjE5UXpvM3pWWnlsR1FRcEI2OHM3NmZBQUc5UWMzTEZ0Y3pJMVpmYk9ZNThzYjVJUkw3MTRT?oc=5\n",
      "\n",
      "Full Description:\n",
      "âš ï¸ No content extracted.\n",
      "====================================================================================================\n",
      "Title: Malaysia closes a back door that may have allowed US-sourced AI chips to reach China - theregister.com\n",
      "Source: theregister.com\n",
      "Date: 2025-07-15 | Time: 01:02:00\n",
      "URL: https://news.google.com/rss/articles/CBMiggFBVV95cUxOVlVuSDVaanBBY0gzd1F6UWYweXVQeHJqbGs0S1dqbTBqbmVoSUI5V3UwUW5RTy1LbndsVHBtMEt0alM1eDJQODhJVDYwUlk1Z3ZtdXVRemlQc3VNQVNWQWN5b2Q5ejlld0lXdWRreUpsY2ppc3BMUElnZmNiTTY5X0xn0gGHAUFVX3lxTFBtTk9nWmoyYnN5eW05UlpSRHFOZWQzT1hxdm1wdVF2UUNSMXBBRGdFV0dfd3V0My1BbjJWQWtHUEtlOTlxOF9yOE5ibjVOT3BfWUM2TUJGMzRQTUdiejZCNlg1a2dmakVTTFZxbm52bXlkd1g4RDIxVFJ1T011V0N2aEtUMXg1QQ?oc=5\n",
      "\n",
      "Full Description:\n",
      "âš ï¸ No content extracted.\n",
      "====================================================================================================\n",
      "Title: NVIDIA CEO Jensen Huang Promotes AI in Washington, DC and China - NVIDIA Blog\n",
      "Source: NVIDIA Blog\n",
      "Date: 2025-07-15 | Time: 01:32:35\n",
      "URL: https://news.google.com/rss/articles/CBMieEFVX3lxTFAtOWRnclExQ19lSEVQX29tRG5GbnAtYkZ3bTdZdFQ4TTFzcnR6Tm1iOXJRN0E1aU82TGNJbWVDaGMwMTBBRU1kSHNtV0NqNUpqVEVvWGdCTkFlRWVKekRPMldLeENWSXg1YnFjbldpTW9qX3AwYmtFUg?oc=5\n",
      "\n",
      "Full Description:\n",
      "âš ï¸ No content extracted.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_real_url_from_summary(summary_html):\n",
    "    soup = BeautifulSoup(summary_html, \"html.parser\")\n",
    "    a_tag = soup.find(\"a\")\n",
    "    return a_tag['href'] if a_tag else None\n",
    "\n",
    "def extract_google_news_articles(query=\"AI technology\"):\n",
    "    query = query.replace(\" \", \"+\")\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={query}&hl=en&gl=US&ceid=US:en\"\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    articles = []\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        try:\n",
    "            title = entry.title\n",
    "            source = entry.source.title if \"source\" in entry else \"Unknown\"\n",
    "            pub_date = datetime(*entry.published_parsed[:6])\n",
    "            date = pub_date.strftime(\"%Y-%m-%d\")\n",
    "            time = pub_date.strftime(\"%H:%M:%S\")\n",
    "\n",
    "            real_url = extract_real_url_from_summary(entry.summary)\n",
    "            if not real_url:\n",
    "                continue\n",
    "\n",
    "            # ðŸ§  Fetch HTML and extract full content using readability\n",
    "            response = requests.get(real_url, headers=headers, timeout=10)\n",
    "            doc = Document(response.text)\n",
    "            full_text_html = doc.summary()\n",
    "            full_text = BeautifulSoup(full_text_html, \"html.parser\").get_text(separator=\"\\n\").strip()\n",
    "\n",
    "            if not full_text:\n",
    "                full_text = \"âš ï¸ No content extracted.\"\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"description\": full_text,\n",
    "                \"source\": source,\n",
    "                \"date\": date,\n",
    "                \"time\": time,\n",
    "                \"url\": real_url\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to extract: {e}\")\n",
    "            continue\n",
    "\n",
    "    return articles\n",
    "\n",
    "# ðŸš€ Use It\n",
    "if __name__ == \"__main__\":\n",
    "    news = extract_google_news_articles(\"AI chip China\")\n",
    "    for a in news[:3]:\n",
    "        print(f\"Title: {a['title']}\")\n",
    "        print(f\"Source: {a['source']}\")\n",
    "        print(f\"Date: {a['date']} | Time: {a['time']}\")\n",
    "        print(f\"URL: {a['url']}\")\n",
    "        print(f\"\\nFull Description:\\n{a['description']}\")\n",
    "        print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19ea4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting readability-lxml\n",
      "  Downloading readability_lxml-0.8.4.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: feedparser in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (6.0.11)\n",
      "Requirement already satisfied: requests in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: chardet in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from readability-lxml) (5.2.0)\n",
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from readability-lxml) (5.3.1)\n",
      "Requirement already satisfied: cssselect in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from readability-lxml) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from beautifulsoup4) (4.13.1)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: lxml_html_clean in c:\\users\\darsh\\miniconda3\\envs\\agno\\lib\\site-packages (from lxml[html_clean]->readability-lxml) (0.4.1)\n",
      "Downloading readability_lxml-0.8.4.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: readability-lxml\n",
      "Successfully installed readability-lxml-0.8.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install readability-lxml beautifulsoup4 feedparser requests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
