{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349f4bbc",
   "metadata": {},
   "source": [
    "# ALl things Extracted except date and time code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from crawl4ai import AsyncWebCrawler, CrawlResult\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# --------- CONFIG --------- #\n",
    "INPUT_FILENAME = \"filtered_news_articles.json\"\n",
    "OUTPUT_FILENAME = \"scraped_article_details.json\"\n",
    "\n",
    "# --- CORE LOGIC --- #\n",
    "\n",
    "def load_articles_from_json(filename: str) -> List[Dict]:\n",
    "    \"\"\"Loads the list of articles from the initial JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[!] Error: Input file '{filename}' not found.\")\n",
    "        print(\"Please run the first script to generate it.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[!] Error: Could not read '{filename}'. Make sure it's a valid JSON file.\")\n",
    "        return []\n",
    "\n",
    "def parse_content_and_datetime(soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Tries to find the publication date/time and the main article content.\n",
    "    This is often the hardest part, as every website is different.\n",
    "    \"\"\"\n",
    "    # 1. Find Date and Time\n",
    "    # Try finding a <time> tag with a 'datetime' attribute (common standard)\n",
    "    datetime_str = \"Not found\"\n",
    "    time_tag = soup.find('time', attrs={'datetime': True})\n",
    "    if time_tag and time_tag.has_attr('datetime'):\n",
    "        datetime_str = time_tag['datetime']\n",
    "    else:\n",
    "        # Fallback: search for date patterns like YYYY-MM-DD in the text\n",
    "        # This regex is a simple example\n",
    "        pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}')\n",
    "        match = pattern.search(soup.get_text())\n",
    "        if match:\n",
    "            datetime_str = match.group(0)\n",
    "\n",
    "    # 2. Find Main Article Content\n",
    "    # Try to find a standard <article> tag first\n",
    "    content = \"Content not found\"\n",
    "    article_body = soup.find('article')\n",
    "    if article_body:\n",
    "        # Join paragraphs with a newline for readability\n",
    "        content = article_body.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        # Fallback: get all text from the page (can be noisy)\n",
    "        # We limit the length to avoid grabbing huge, irrelevant text blocks\n",
    "        all_text = soup.get_text(separator='\\n', strip=True)\n",
    "        if len(all_text) > 200: # Basic check to see if there's meaningful content\n",
    "             content = all_text\n",
    "\n",
    "    return {\n",
    "        \"scraped_datetime\": datetime_str,\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "\n",
    "async def scrape_single_article(article_info: Dict, crawler: AsyncWebCrawler) -> Dict:\n",
    "    \"\"\"Scrapes one URL and extracts the required details.\"\"\"\n",
    "    url = article_info.get(\"url\")\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    print(f\"-> Scraping {url}\")\n",
    "    result: CrawlResult = await crawler.arun(url)\n",
    "\n",
    "    if not result.success or not result.cleaned_html:\n",
    "        print(f\"[!] Failed to crawl {url}\")\n",
    "        return {**article_info, \"scraped_datetime\": \"Crawl Failed\", \"content\": \"Crawl Failed\"}\n",
    "\n",
    "    soup = BeautifulSoup(result.cleaned_html, \"html.parser\")\n",
    "    \n",
    "    # Get the date, time, and content\n",
    "    extracted_data = parse_content_and_datetime(soup)\n",
    "\n",
    "    # Combine the original info with the newly scraped data\n",
    "    return {**article_info, **extracted_data}\n",
    "\n",
    "\n",
    "# --- MAIN FUNCTION --- #\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to orchestrate the scraping process.\"\"\"\n",
    "    articles_to_scrape = load_articles_from_json(INPUT_FILENAME)\n",
    "\n",
    "    if not articles_to_scrape:\n",
    "        return # Stop if there's nothing to do\n",
    "\n",
    "    print(f\"✅ Found {len(articles_to_scrape)} articles to scrape from '{INPUT_FILENAME}'.\\n\")\n",
    "\n",
    "    # Use the crawler within a context manager\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        tasks = [scrape_single_article(article, crawler) for article in articles_to_scrape]\n",
    "        all_results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Filter out any potential failures\n",
    "    successful_results = [res for res in all_results if res is not None]\n",
    "\n",
    "    # Save the detailed results to a new JSON file\n",
    "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "        json.dump(successful_results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n✅ Done! Saved details for {len(successful_results)} articles to '{OUTPUT_FILENAME}'.\")\n",
    "\n",
    "\n",
    "# --- RUNNER --- #\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979d495",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
